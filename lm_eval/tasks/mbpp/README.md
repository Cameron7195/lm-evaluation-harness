# MBPP

### Paper

Title: `"Program Synthesis with Large Language Models"`
https://arxiv.org/pdf/2108.07732

This paper investigates the capabilities of large language models (LLMs) in synthesizing Python programs. The study evaluates models with varying parameter sizes (from 244M to 137B) on two novel benchmarks, MBPP and MathQA-Python. The MBPP dataset consists of 974 Python tasks suitable for entry-level programmers, while MathQA-Python is a Python adaptation of the MathQA benchmark with over 23k tasks. The study reveals that performance scales log-linearly with model size. Fine-tuning on a small dataset of 374 problems improves performance, with the largest model solving 59.6% of MBPP tasks using few-shot learning. The study also explores the modelsâ€™ ability to engage in code-related dialog and their limitations in understanding program semantics.

WARNING: This task executes arbitrary code generated by an LLM unsafely.

Homepage: https://github.com/google-research/google-research/tree/master/mbpp

### Citation
@article{austin2021program,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

### Groups and Tasks

#### Groups

* Not part of a group yet.

#### Tasks

* mbpp_3shot_llama
    - This task evaluates 3-shot python programs using the same prompt setup and format specified by meta in this document: https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-Instruct-evals/viewer/Meta-Llama-3.1-8B-Instruct-evals__mbpp__details?row=0


### Checklist

For adding novel benchmarks/datasets to the library:
* [x] Is the task an existing benchmark in the literature?
  * [x] Have you referenced the original paper that introduced the task?
  * [ ] If yes, does the original paper provide a reference implementation? If so, have you checked against the reference implementation and documented how to run such a test?


If other tasks on this dataset are already supported:
* [ ] Is the "Main" variant of this task clearly denoted?
* [x] Have you provided a short sentence in a README on what each new variant adds / evaluates?
* [x] Have you noted which, if any, published evaluation setups are matched by this variant?
