{
  "results": {
    "csqa_cot_llama": {
      "alias": "csqa_cot_llama",
      "exact_match,strict-match": 0.78125,
      "exact_match_stderr,strict-match": 0.03668319762764659,
      "exact_match,flexible-extract": 0.796875,
      "exact_match_stderr,flexible-extract": 0.035700551879873775
    }
  },
  "group_subtasks": {
    "csqa_cot_llama": []
  },
  "configs": {
    "csqa_cot_llama": {
      "task": "csqa_cot_llama",
      "tag": [
        "chain_of_thought"
      ],
      "dataset_path": "tau/commonsense_qa",
      "training_split": "train",
      "validation_split": "validation",
      "doc_to_text": "def doc_to_text(doc):\n    question = doc[\"question\"]\n    choices = doc[\"choices\"]\n    \n    #mc = \" \".join([f\"({label}) {text}\" for label, text in zip(choices['label'], choices['text'])])\n    #return f\"Given the following problem, reason and give a final answer to the problem.\\nProblem: {question} {mc}\\nYour response should end with \\\"The final answer is [answer]\\\" where [answer] is the response to the problem.\\n\"\n\n    mc = \"\\n\".join([f\"{label}. {text}\" for label, text in zip(choices['label'], choices['text'])])\n    return f\"Given the following problem, reason and give a final answer to the problem.\\nProblem: {question.strip()}\\n{mc}\\nYour response should end with \\\"The final answer is [answer]\\\" where [answer] is the response to the problem.\\n\"\n",
      "doc_to_target": "{{answerKey if answerKey is defined else target}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "samples": [
          {
            "question": "What do people use to absorb extra ink from a fountain pen?",
            "choices": {
              "text": [
                "shirt pocket",
                "calligrapherâ€™s hand",
                "inkwell",
                "desk drawer",
                "blotter"
              ],
              "label": [
                "A",
                "B",
                "C",
                "D",
                "E"
              ]
            },
            "target": "The answer must be an item that can absorb ink. Of the above choices, only blotters are used to absorb ink. The final answer is E."
          },
          {
            "question": "What home entertainment equipment requires cable?",
            "choices": {
              "text": [
                "radio shack",
                "substation",
                "television",
                "cabinet"
              ],
              "label": [
                "A",
                "B",
                "C",
                "D"
              ]
            },
            "target": "The answer must require cable. Of the above choices, only television requires cable. The final answer is C."
          },
          {
            "question": "The fox walked from the city into the forest, what was it looking for?",
            "choices": {
              "text": [
                "pretty flowers",
                "hen house",
                "natural habitat",
                "storybook"
              ],
              "label": [
                "A",
                "B",
                "C",
                "D"
              ]
            },
            "target": "The answer must be something in the forest. Of the above choices, only natural habitat is in the forest. The final answer is C."
          },
          {
            "question": "Sammy wanted to go to where the people were. Where might he go?",
            "choices": {
              "text": [
                "populated areas",
                "race track",
                "desert",
                "apartment",
                "roadblock"
              ],
              "label": [
                "A",
                "B",
                "C",
                "D",
                "E"
              ]
            },
            "target": "The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of people. The final answer is A."
          },
          {
            "question": "Where do you put your grapes just before checking out?",
            "choices": {
              "text": [
                "mouth",
                "grocery cart",
                "super market",
                "fruit basket",
                "fruit market"
              ],
              "label": [
                "A",
                "B",
                "C",
                "D",
                "E"
              ]
            },
            "target": "The answer should be the place where grocery items are placed before checking out. Of the above choices, grocery cart makes the most sense for holding grocery items. The final answer is B."
          },
          {
            "question": "Google Maps and other highway and street GPS services have replaced what?",
            "choices": {
              "text": [
                "united states",
                "mexico",
                "countryside",
                "atlas"
              ],
              "label": [
                "A",
                "B",
                "C",
                "D"
              ]
            },
            "target": "The answer must be something that used to do what Google Maps and GPS services do, which is to give directions. Of the above choices, only atlases are used to give directions. The final answer is D."
          },
          {
            "question": "Before getting a divorce, what did the wife feel who was doing all the work?",
            "choices": {
              "text": [
                "harder",
                "anguish",
                "bitterness",
                "tears",
                "sadness"
              ],
              "label": [
                "A",
                "B",
                "C",
                "D",
                "E"
              ]
            },
            "target": "The answer should be the feeling of someone getting divorced who was doing all the work. Of the above choices, the closest feeling is bitterness. The final answer is C."
          }
        ]
      },
      "num_fewshot": 7,
      "metric_list": [
        {
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": false,
          "metric": "exact_match",
          "regexes_to_ignore": [
            ",",
            "\\$",
            "(?s).*#### ",
            "\\.$"
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "do_sample": false,
        "until": [
          "<|eot_id|>",
          "<|start_header_id|>user<|end_header_id|>",
          "Q:",
          "</s>",
          "<|im_end|>"
        ],
        "max_gen_toks": 1024
      },
      "repeats": 1,
      "filter_list": [
        {
          "filter": [
            {
              "function": "regex",
              "group_select": -1,
              "regex_pattern": "The final answer is ([a-zA-Z1-9])"
            },
            {
              "function": "take_first"
            }
          ],
          "name": "strict-match"
        },
        {
          "filter": [
            {
              "function": "regex",
              "group_select": -1,
              "regex_pattern": "([a-zA-Z1-9])"
            },
            {
              "function": "take_first"
            }
          ],
          "name": "flexible-extract"
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    }
  },
  "versions": {
    "csqa_cot_llama": 1.0
  },
  "n-shot": {
    "csqa_cot_llama": 7
  },
  "higher_is_better": {
    "csqa_cot_llama": {
      "exact_match": true
    }
  },
  "n-samples": {
    "csqa_cot_llama": {
      "original": 1221,
      "effective": 128
    }
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=meta-llama/Meta-Llama-3-8B-Instruct",
    "model_num_parameters": 8030261248,
    "model_dtype": "torch.bfloat16",
    "model_revision": "main",
    "model_sha": "e1945c40cd546c78e41f1151f4db032b271faeaa",
    "batch_size": "32",
    "batch_sizes": [],
    "device": "cuda:0",
    "use_cache": null,
    "limit": 128.0,
    "bootstrap_iters": 100000,
    "gen_kwargs": {
      "max_gen_toks": 1024
    },
    "random_seed": 42,
    "numpy_seed": 42,
    "torch_seed": 42,
    "fewshot_seed": 42
  },
  "git_hash": "9b303f2d",
  "date": 1724379088.4846833,
  "pretty_env_info": "PyTorch version: 2.3.1\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] (64-bit runtime)\nPython platform: Linux-5.15.0-118-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.5.82\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3\nNvidia driver version: 550.90.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               22\nOn-line CPU(s) list:                  0-21\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8470\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   11\nSocket(s):                            1\nStepping:                             8\nBogoMIPS:                             4000.11\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch topoext cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk avx512_fp16 arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            704 KiB (22 instances)\nL1i cache:                            704 KiB (22 instances)\nL2 cache:                             44 MiB (11 instances)\nL3 cache:                             16 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-21\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Unknown: No mitigations\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] torchaudio==2.3.1\n[pip3] torchvision==0.18.1\n[pip3] triton==2.3.1\n[conda] blas                      2.116                       mkl    conda-forge\n[conda] blas-devel                3.9.0            16_linux64_mkl    conda-forge\n[conda] libblas                   3.9.0            16_linux64_mkl    conda-forge\n[conda] libcblas                  3.9.0            16_linux64_mkl    conda-forge\n[conda] liblapack                 3.9.0            16_linux64_mkl    conda-forge\n[conda] liblapacke                3.9.0            16_linux64_mkl    conda-forge\n[conda] libopenvino-pytorch-frontend 2024.2.0             he02047a_1    conda-forge\n[conda] mkl                       2022.1.0           h84fe81f_915    conda-forge\n[conda] mkl-devel                 2022.1.0           ha770c72_916    conda-forge\n[conda] mkl-include               2022.1.0           h84fe81f_915    conda-forge\n[conda] nomkl                     2.0                           0    anaconda\n[conda] numpy                     1.26.4          py311h64a7726_0    conda-forge\n[conda] pytorch                   2.3.1           py3.11_cuda12.1_cudnn8.9.2_0    pytorch\n[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch\n[conda] pytorch-mutex             1.0                        cuda    pytorch\n[conda] torchaudio                2.3.1               py311_cu121    pytorch\n[conda] torchtriton               2.3.1                     py311    pytorch\n[conda] torchvision               0.18.1              py311_cu121    pytorch",
  "transformers_version": "4.44.0",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<|eot_id|>",
    "128009"
  ],
  "tokenizer_eos_token": [
    "<|eot_id|>",
    "128009"
  ],
  "tokenizer_bos_token": [
    "<|begin_of_text|>",
    "128000"
  ],
  "eot_token_id": 128009,
  "max_length": 8192,
  "task_hashes": {
    "csqa_cot_llama": "3032ab72a4fe97c09556f540baa7f63206e4a7471205054e20d6e7908268b77f"
  },
  "model_source": "hf",
  "model_name": "meta-llama/Meta-Llama-3-8B-Instruct",
  "model_name_sanitized": "meta-llama__Meta-Llama-3-8B-Instruct",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": true,
  "chat_template": "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}",
  "chat_template_sha": "ba03a121d097859c7b5b9cd03af99aafe95275210d2876f642ad9929a150f122",
  "start_time": 33540.385298351,
  "end_time": 33579.577129473,
  "total_evaluation_time_seconds": "39.19183112199971"
}